# Cleaning Data with PySpark

## 1. DataFrame Details

### 1.1 Intro to data cleaning with Apache Spark
* Data Cleaning is defined as preparing raw data for use in processing pipelines
* Possible tasks in data cleaning include: reformatting or replacing text; performing calculations based on the data; and removing garbage or incomplete data

1.1.1 Spark Schemas
* A primary function of data cleaning is to verify all data is in the expected format.
* Spark provides a built-in ability to validate datasets with schemas.
* A schema defines and validates the number and types of columns for a given DataFrame.
* A schema can contain many different types of fields - integers, floats, dates, strings, and even arrays or mapping structures.
* A defined schema allows Spark to filter out data that doesn't conform during read, ensuring expected correctness.
* In addition, schemas also have performance benefits. Normally a data import will try to infer a schema on read - this requires reading the data twice. Defining a schema limits this to a single read operation.
* Example Spark Schema:
   * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/649f0b6f-7e91-4cdc-b237-6c8eb6940e3b)
