# Part 1: Cleaning Data with PySpark

## 1. Intro to data cleaning with Apache Spark
* Data Cleaning is defined as preparing raw data for use in processing pipelines
* Possible tasks in data cleaning include: reformatting or replacing text; performing calculations based on the data; and removing garbage or incomplete data

### 1.1 Spark Schemas
* A primary function of data cleaning is to verify all data is in the expected format.
* Spark provides a built-in ability to validate datasets with schemas.
* A schema defines and validates the number and types of columns for a given DataFrame.
* A schema can contain many different types of fields - integers, floats, dates, strings, and even arrays or mapping structures.
* A defined schema allows Spark to filter out data that doesn't conform during read, ensuring expected correctness.
* In addition, schemas also have performance benefits. Normally a data import will try to infer a schema on read - this requires reading the data twice. Defining a schema limits this to a single read operation.
* Example Spark Schema:
   * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/649f0b6f-7e91-4cdc-b237-6c8eb6940e3b)

### 1.2 Immutability and Lazy Processing
* Normally in Python, and most other languages, variables are fully mutable. The values can be changed at any given time, assuming the scope of the variable is valid.
* While very flexible, this does present problems anytime there are multiple concurrent components trying to modify the same data.
* Most languages work around these issues using constructs like mutexes, semaphores, etc. This can add complexity, especially with non-trivial programs.
* Unlike typical Python variables, Spark Data Frames are immutable.
  * While not strictly required, immutability is often a component of functional programming
  * Thus, Spark is designed to use immutable objects. Practically, this means Spark Data Frames are defined once and are not modifiable after initialization.
  * If the variable name is reused, the original data is removed (assuming it's not in use elsewhere) and the variable name is reassigned to the new data.
  * While this seems inefficient, it actually allows Spark to share data between all cluster components. It can do so without worry about concurrent data objects.
* Spark's strengths come from a concept called lazy processing/execution:
  * Lazy processing in Spark is the idea that very little actually happens until an action is performed.
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/dfa7f4d3-3004-4f65-b9c6-c1f39cbea86d)

### 1.3 Understanding Parquet Format
* Spark can read in text and CSV files. While this gives us access to many data sources, it's not always the most convenient format to work with.
* Some common issues with CSV files include:
  * The schema is not defined: there are no data types included, nor column names (beyond a header row).
  * Using content containing a comma (or another delimiter) requires escaping.
  * Using the escape character within content requires even further escaping.
  * The available encoding formats are limited depending on the language used

(a) Spark and CSV Files Issues
* In addition to the issues with CSV files in general, Spark has some specific problems processing CSV data.
* CSV files are quite slow to import and parse
  * The files cannot be shared between workers during the import process.
  * If no schema is defined, all data must be read before a schema can be inferred.
* You cannot filter the CSV data via predicate pushdown
  * Spark has feature known as predicate pushdown. Basically, this is the idea of ordering tasks to do the least amount of work.
  * Filtering data prior to processing is one of the primary optimizations of predicate pushdown.
  * This drastically reduces the amount of information that must be processed in large data sets.
* Finally, Spark processes are often multi-step and may utilize an intermediate file representation.
  * These representations allow data to be used later without regenerating the data from source.
  * Using CSV would instead require a significant amount of extra work defining schemas, encoding formats, etc.

(b) The Parquet Format
* Parquet is a compressed columnar data format developed for use in any Hadoop based system. This includes Spark, Hadoop, Apache Impala, and so forth.
* The Parquet format is structured with data accessible in chunks, allowing efficient read / write operations without processing the entire file.
* This structured format supports Spark's predicate pushdown functionality, providing significant performance improvement.
* Finally, Parquet files automatically include schema information and handle data encoding. This is perfect for intermediary or on-disk representation of processed data.
* Note that Parquet files are a binary file format and can only be used with the proper tools. This is in contrast to CSV files which can be edited with any text editor

(c) Working with Parquet 
* Interacting with Parquet files is very straightforward.
* To read a parquet file into a Data Frame, you have two options. The first is using the `spark.read.format` method we've seen previously: df=spark.read.format('parquet').load('filename.parquet')
* The second option is the shortcut version: df=spark.read.parquet('filename.parquet')
* Typically, the shortcut version is the easiest to use but you can use them interchangeably.
* Writing parquet files is similar, using either: df.write.format('parquet').save('filename.parquet') or df.write.parquet('filename.parquet')
* The long-form versions of each permit extra option flags, such as when overwriting an existing parquet file.
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/08095fa6-7198-4be0-9d7e-839f2b31093c)

(d) Parquet and SQL
* Parquet files have various uses within Spark. We've discussed using them as an intermediate data format, but they also are perfect for performing SQL operations.
* To perform a SQL query against a Parquet file, we first need to create a Data Frame via the spark.read.parquet method.
* Once we have the Data Frame, we can use the createOrReplaceTempView() method to add an alias of the Parquet data as a SQL table.
* Finally, we run our query using normal SQL syntax and the spark.sql method.
* In this case, we're looking for all flights with a duration under 100 minutes:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/5e6b7ac5-ea7c-4afe-a1a8-1fe81f147eed)
* Because we're using Parquet as the backing store, we get all the performance benefits we've discussed previously (primarily defined schemas and the available use of predicate pushdown).


## 2. Manipulating DataFrames in the Real World

### 2.1 DataFrame column operations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/fdc4f209-0920-4f6f-89f8-fb93ef7bc0df)
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/79d44e93-c3f8-4d5d-9282-3f1ba461644c)
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/ddc8955b-1e29-41b0-9ce3-036aa7a040d1)
* Some of the most common operations used in data cleaning are modifying and converting strings
* You will typically apply these to each column as a transformation.
* Many of these functions are in the pyspark.sql.functions library. For brevity, we'll import it as the alias 'F':
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/265c2ac2-90a7-44f4-a8a7-7fac8cec773d)
* Example test:
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4c743a3e-3098-4317-948f-a0c5ffbec79d)

ArrayType() column functions
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/9f95ea8a-847f-4f5c-9442-ebad7d44f067)
* Example Test:
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/f791d959-0dfe-45b8-831d-92dfc8bcb45d)

### 2.2 Conditional DataFrame column operations
* The DataFrame transformations we've covered thus far are blanket transformations, meaning they're applied regardless of the data.
* Often you'll want to conditionally change some aspect of the contents.
* Spark provides some built in conditional clauses which act similar to an if / then / else statement in a traditional programming environment.
* There are two components to the conditional clauses: .when(), and the optional .otherwise()
* The .when() clause is a method available from the pyspark.sql.functions library that is looking for two components: the if condition, and what to do if it evaluates to true:
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/80f26645-6246-4251-8c83-49e0ac1bd09d)
* You can chain multiple when statements together, similar to an if / else if structure. You can chain as many when clauses together as required.
* In this case, we define two .when() clauses and return Adult or Minor based on the Age column:
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/810ef2c3-0a0f-4ceb-8e7e-18ba9fd9705e)
* The otherwise() clause:
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/d651d52a-ee25-4fb0-9548-7a362d21eac3)

### 2.3 User Defined Functions
* A user defined function, or UDF, is a Python method that the user writes to perform a specific bit of logic.
* Once written, the method is called via the pyspark.sql.functions.udf() method.
* The result is stored as a variable and can be called as a normal Spark function
* Simple example:
  * First, we define a python function. We'll call our function, reverseString(), with an argument called mystr. We'll use some python shorthand to reverse the string and return it.
  * The next step is to wrap the function and store it in a variable for later use.
  * We'll use the pyspark.sql.functions.udf() method. It takes two arguments - the name of the method you just defined, and the Spark data type it will return.
  * This can be any of the options in pyspark.sql.types, and can even be a more complex type, including a fully defined schema object
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/fb373c07-b9f8-423e-b221-a41c24aecfdb)
* Argument-less function example:
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/cfc503e5-e93a-4afb-bf99-97f5db4c5424)




