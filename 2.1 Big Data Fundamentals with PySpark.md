# Part 2: Big Data Fundamentals - PySpark SQL & DataFrames
* PySpark SQL is a Spark module for structured data processing.
* It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine
* Unlike the PySpark RDD API, PySpark SQL provides more information about the structure of data and the computation being performed

## 1. Abstracting Data with PySpark DataFrames 
* A DataFrame is an immutable distributed collection of data with named columns. It is similar to a table in SQL.
* DataFrames are designed to process a large collection of structured data such as relational database and semi-structured data such as JSON
* DataFrame API currently supports several languages such as Python, R, Scala, and Java.
* DataFrames allows PySpark to query data using SQL, for example (SELECT * from table) or using the expression method for example (df-dot-select).

### 1.1 SparkSession - Entry Point for DataFrame API
* Previously you have learned about SparkContext which is the main entry point for creating RDDs.
* Similarly, SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame API.
* The SparkSession does for DataFrames what the SparkContext does for RDDs.
* A SparkSession can be used to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables etc.
* Similar to SparkContext, SparkSession is exposed to the PySpark shell as variable spark

### 1.2 Creating DataFrames in PySpark
* DataFrames in PySpark can be created in two main ways:
    * From an existing RDD using SparkSession's createDataFrame() method
    * From different data sources such as CSV, JSON, TXT using SparkSession's read method.
* Before going into the details of creating DataFrames, let's understand what schema is.
* Schema is the structure of data in DataFrame and helps Spark to optimize queries on the data more efficiently.
* A schema provides informational detail such as the column name, the type of data in that column, and whether null or empty values are allowed in the column.

(a) Create a DataFrame from RDD
* To create a DataFrame from an RDD, we will need to pass an RDD and a schema into SparkSession's createDataFrame method
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/d060f529-fbbb-48e2-be0c-a8d2df88305a)

(b) Create a DataFrame from reading CSV/JSON/TXT files
* To create a DataFrame from CSV/JSON/TXT files, we will make use of the SparkSession's spark.read property. Here is an example of creating df_csv DataFrame from people.csv file using spark.read.csv method:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/c95357f2-2837-4b40-81b6-940fe6cfecb3)

## 2. Operating on DataFrames in PySpark
* Just like RDDs, DataFrames also support both transformations and actions.
* Let's explore some of the most common DataFrame Transformations such as select, filter, groupby, orderby, dropDuplicates, withColumnRenamed
* We will also look at some common DataFrame Actions such as printSchema, show, count, columns and describe

(a) select() and show() operations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/412a091c-cec3-4709-b3b3-cf3ca02ce416)

(b) groupby() and count() operations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/5a7e3d11-6a8f-4410-a214-78fd22a3c171)

(c) orderBy() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0474a930-25d7-47a3-9073-d3c798fa8ae6)

(d) dropDuplicates() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0cd86e17-ab1e-4665-baee-68d73085be7c)

(e) withColumnRenamed() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0feb768b-0e02-4b27-962a-893203eedc9f)

(f) printSchema() Action
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4817b4fc-643b-4d50-beb3-3a6d4c99053d)

(g) columns Action
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4561b8b8-79cc-4ff6-88f3-e92df481a2d8)

(h) describe() Action
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/1a7c5eee-9670-40ef-bb10-529cd1d1b23e)

(i) filter()
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/ffbba71c-58a1-43dc-8b4f-f77319ec5364)
 
## 3. Interacting with DataFrames using PySpark SQL
* Previously, you have seen how to interact with PySparkSQL using DataFrame API
* Next, we explore how to interact with PySparkSQL using SQL query

### 3.1 DataFrame API vs SQL queries
* In addition to DataFrame API, PySpark SQL allows you to manipulate DataFrames with SQL queries.
* What you can do using DataFrames API, can be done using SQL queries and vice versa.
* So what are the differences between DataFrames API and SQL queries?
* The DataFrames API provides a programmatic interface – basically a domain-specific language (DSL) for interacting with data.
* DataFrame queries are much easier to construct programmatically.
* Plain SQL queries can be significantly more concise and easier to understand. They are also portable and can be used without any modifications with every supported language.
* Many of the DataFrame operations that you have seen in the previous chapter, can be done using SQL queries.

### 3.2 Executing SQL Queries
* The SparkSession provides a method called sql which can be used to execute a SQL query.
* The sql method takes a SQL statement as an argument and returns a DataFrame representing the result of the given query:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0560f8f0-b281-462b-9010-6d7fa888ed40)

Example Queries:

(a) SQL query to extract data
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/8637f543-644d-44a9-977a-40604ebb0063)

(b) Summarizing and Grouping Data using SQL Query
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/61265166-619a-41c6-8557-e15711746100)

(c) Filtering Columns using SQL Query
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/7eee8539-aab5-401e-beb2-6840730b8dbb)

## 4. Data Visualization in PySpark using DataFrames
* Data visualization is the way of representing your data in form of graphs or charts. It is considered a crucial component of Exploratory Data Analysis (EDA).
* Several open source tools exist to aid visualization in Python such as matplotlib, Seaborn, Bokeh etc.
* However, none of these visualization tools can be used directly with PySpark's DataFrames.
*  Currently, there are three different methods available to create charts using PySpark DataFrames - pyspark_dist_explore library, toPandas method, and HandySpark toPandas

(a) Data Visualization using Pyspark_dist_explore 
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/9e35a63f-3a17-42bc-9fcf-8636657827ef)

(b) Using Pandas for plotting DataFrames: toPandas()
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/05bcb026-4707-433e-b2e5-d76b6d13ca68)

Before we look at the third method, let's take a look at the differences between Pandas vs Spark DataFrames: 
   * But, Pandas won’t work in every case. It is a single machine tool and constrained by single machine limits.
   * So their size is limited by your server memory, and you will process them with the power of a single server.
   * In contrast, operations on Pyspark DataFrames run parallel on different nodes in the cluster.
   * In pandas DataFrames, we get the result as soon as we apply any operation Whereas operations in PySpark DataFrames are lazy in nature.
   * You can change a Pandas DataFrame using methods. We can’t change a PySpark DataFrame due to its immutable property.
   * Finally, the Pandas API supports more operations than PySpark DataFrames.

(c) HandySpark method of visualization
* HandySpark libary is a relatively a new package.
* HandySpark is designed to improve PySpark user experience, especially when it comes to exploratory data analysis, including visualization capabilities.
* It makes fetching data or computing statistics for columns really easy, returning pandas objects straight away.
* It brings the long-missing capability of plotting data while retaining the advantage of performing the distributed computation:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/5aea62c6-2870-4e12-b08d-82065157b94a)


## 4. Machine Learning with PySpark MLlib
* PySpark MLlib is the Apache Spark scalable machine learning library in Python consisting of common learning algorithms and utilities
* At a high level, PySpark MLlib provides tools such as:
   * Machine learning algorithms which include collaborative filtering, classification, and clustering
   * Featurization which include feature extraction, transformation, dimensionality reduction, and selection
   * Pipelines which include constructing, evaluating, and tuning ML Pipelines
* Note that pyspark.mllib can only support RDDs, thus you have to change DataFrames to RDDs
* Why PySpark MLlib?
   * Scikit-learn algorithms work well for small to medium-sized datasets that can be processed on a single machine, but not for large datasets that require the power of parallel processing
   * On the other hand, PySpark MLlib only contains algorithms in which operations can be applied in parallel across nodes in a cluster
   * Unlike Scikit-learn, MLlib supports several other higher languages such as Scala, Java, and R in addition to Python
   * MLlib also provides a high-level API to build machine-learning pipelines. A machine learning pipeline is a complete workflow combining multiple machine learning algorithms together

### 4.1 PySpark MLlib Algorithms
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0796ae43-9761-42e6-85f7-6d4a87e6ec25)
* While PySpark MLlib includes several ML algos, we will specifically focus on the three key areas, often referred to as the three Cs of machine learning - Collaborative filtering, Classification, and Clustering:
   * Collaborative filtering produces recommendations based on past behavior, preferences, or similarities to known entities/users
   * Classification is the problem of identifying to which of a set of categories a new observation belongs
   * Clustering is grouping of data into clusters based on similar characteristics. 
* To import some of the PySpark MLlib libraries in the PySpark shell environment:
   * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/96227618-b371-427c-bfac-0cd077fee085)

### 4.2 Collaborative Filtering
* Collaborative filtering is a method of making automatic predictions about the interests of a user by collecting preferences or taste information from many users
* Collaborative filtering is one of the most commonly used algorithms in recommender systems
* Collaborative filtering has two approaches: The User-User approach and Item-Item approach
* The User-User approach finds users that are similar to the target user and uses their collaborative ratings to make recommendations for the target user
* Item-Item approach finds and recommends items that are similar or related to items associated with the target user

Steps in Modeling:
(a) Rating class in pyspark.mllib.recommendation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/9bba488b-6a3c-4726-9153-fd13ea89eef5)

(b) Splitting the data using randomSplit()
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/a816b382-7a69-4fea-a9f0-35849b171eab)

(c) Model Building: Alternating Least Squares (ALS)
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/5c698e2f-8e03-42a5-a2da-653cc76b3a31)

(d) Getting Predictions: predictAll()
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/82a3e208-de71-47b1-9535-337b491a02aa)

(e) Model Evaluation using Mean Squared Error (MSE)
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/daf31af6-e50d-4ab7-9027-1c5855bed490)

Exercise Problem: Simple Movie Recommendation System using a subset of MovieLens 100k dataset
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/9ddcb235-e4bd-4d7c-a57e-b0d38af57c8e)
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/bc8de715-86ee-4900-8084-cd24f1489739)
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/1f5f24e1-2cc2-4522-97d8-592b238216a3)

### 4.3 Classification
* Classification is a popular machine learning algorithm that identifies which category an item belongs to. For example, whether an email is spam or non-spam, based on labeled examples of other items.
* Classification takes a set of data with known labels and pre-determined features and learns how to label new records based on that information
* That is why Classification comes under a supervised learning technique.
* Classifications can be divided into two different types - Binary Classification and Multiclass Classification:
   * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/260cc8a8-9738-4754-a1fd-47dec273952a)
* We will focus on Logistic Regression which is the most popular Classification method:
   * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/3961c3df-77e5-411a-af01-b1a879aaf732)
* PySpark MLlib contains a few specific data types such as Vectors and LabeledPoint

(a) Working with Vectors in PySpark MLlib
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/a1da0027-c503-4b86-9685-cdcc8b80e011)

(b) LabeledPoint() in in PySpark MLlib
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4779bf10-93f9-4ad1-87d0-32e3b4878fc1)

(c) HashingTF() in PySpark MLlib 
* PySpark MLlib has an algorithm called HashingTF that computes a term frequency vector of a given size from a document.
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4d7b3bee-b6e1-4fce-ba12-9abe45eaf2b1)

(d) Logistic Regression using LogisticRegressionWithLBFGS
* Among several algorithms, the popular algorithm available for Logistic Regression in PySpark MLlib is LBFGS
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/a5172768-f2f0-4658-b13e-2484dd729b92)

Exercise Problem: Email Spam Classifier with Logistic Regression using Spark MLlib
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/9f5383a6-0a5f-44c2-a153-92f85ad792c3)
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/b7da08f3-06aa-4263-8161-45c2bd08388e)
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/514ef45a-4b4e-42d5-827d-d3bcc9b5250d)

### 4.4 Clustering
* Clustering is the unsupervised learning task that involves grouping unlabeled data together into clusters of high similarity
* Unlike the supervised learning methods that you have seen before such as Collaborative filtering and Classification, where data is labeled, Clustering can be used to make sense of unlabeled data
* PySpark MLlib library offers a handful of clustering models such as K-means clustering, Gaussian mixture clustering, Power iteration clustering (PIC), Bisecting k-means clustering and Streaming k-means clustering
* We will focus on K-means clustering because of its simplicity and popularity.

K-Means Clustering
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/f5769f92-24a9-48de-a1a8-f09dc88a28c4)

Steps in Modeling:
(a) Loading data into RDD
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/46dcb68a-97ed-4d79-bf5a-cba886696b99)

(b) Train a K-Means Clustering Model
* Like other algorithms, you invoke K-means by calling KMeans.train() which takes an RDD, the number of clusters we expect, and the maximum number of iterations allowed
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/33ff801c-972d-4542-b81c-6bbc95b9c0aa)

(c) Evaluating the K-Means Clustering Model
* The next step in K-means clustering is to evaluate the model by computing the error function
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/8eaa804a-5020-4fd9-9e0d-c5049edea408)

(d) Visualizing K-means clusters
* An optional but highly recommended step in K-means clustering is cluster visualization
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/06422aff-573e-41ae-94e2-33aca879dae8)
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/cf4700e9-61e6-4647-852b-2a941bda5274)

