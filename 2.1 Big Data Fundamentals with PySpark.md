# Part 2: Big Data Fundamentals - PySpark SQL & DataFrames
* PySpark SQL is a Spark module for structured data processing.
* It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine
* Unlike the PySpark RDD API, PySpark SQL provides more information about the structure of data and the computation being performed

## 1. Abstracting Data with PySpark DataFrames 
* A DataFrame is an immutable distributed collection of data with named columns. It is similar to a table in SQL.
* DataFrames are designed to process a large collection of structured data such as relational database and semi-structured data such as JSON
* DataFrame API currently supports several languages such as Python, R, Scala, and Java.
* DataFrames allows PySpark to query data using SQL, for example (SELECT * from table) or using the expression method for example (df-dot-select).

### 1.1 SparkSession - Entry Point for DataFrame API
* Previously you have learned about SparkContext which is the main entry point for creating RDDs.
* Similarly, SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame API.
* The SparkSession does for DataFrames what the SparkContext does for RDDs.
* A SparkSession can be used to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables etc.
* Similar to SparkContext, SparkSession is exposed to the PySpark shell as variable spark

### 1.2 Creating DataFrames in PySpark
* DataFrames in PySpark can be created in two main ways:
    * From an existing RDD using SparkSession's createDataFrame() method
    * From different data sources such as CSV, JSON, TXT using SparkSession's read method.
* Before going into the details of creating DataFrames, let's understand what schema is.
* Schema is the structure of data in DataFrame and helps Spark to optimize queries on the data more efficiently.
* A schema provides informational detail such as the column name, the type of data in that column, and whether null or empty values are allowed in the column.

(a) Create a DataFrame from RDD
* To create a DataFrame from an RDD, we will need to pass an RDD and a schema into SparkSession's createDataFrame method
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/d060f529-fbbb-48e2-be0c-a8d2df88305a)

(b) Create a DataFrame from reading CSV/JSON/TXT files
* To create a DataFrame from CSV/JSON/TXT files, we will make use of the SparkSession's spark.read property. Here is an example of creating df_csv DataFrame from people.csv file using spark.read.csv method:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/c95357f2-2837-4b40-81b6-940fe6cfecb3)

## 2. Operating on DataFrames in PySpark
* Just like RDDs, DataFrames also support both transformations and actions.
* Let's explore some of the most common DataFrame Transformations such as select, filter, groupby, orderby, dropDuplicates, withColumnRenamed
* We will also look at some common DataFrame Actions such as printSchema, show, count, columns and describe

(a) select() and show() operations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/412a091c-cec3-4709-b3b3-cf3ca02ce416)

(b) groupby() and count() operations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/5a7e3d11-6a8f-4410-a214-78fd22a3c171)

(c) orderBy() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0474a930-25d7-47a3-9073-d3c798fa8ae6)

(d) dropDuplicates() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0cd86e17-ab1e-4665-baee-68d73085be7c)

(e) withColumnRenamed() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0feb768b-0e02-4b27-962a-893203eedc9f)

(f) printSchema() Action
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4817b4fc-643b-4d50-beb3-3a6d4c99053d)

(g) columns Action
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4561b8b8-79cc-4ff6-88f3-e92df481a2d8)

(h) describe() Action
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/1a7c5eee-9670-40ef-bb10-529cd1d1b23e)
