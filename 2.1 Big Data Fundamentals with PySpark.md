# Part 2: Big Data Fundamentals - PySpark SQL & DataFrames
* PySpark SQL is a Spark module for structured data processing.
* It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine
* Unlike the PySpark RDD API, PySpark SQL provides more information about the structure of data and the computation being performed

## 1. Abstracting Data with PySpark DataFrames 
* A DataFrame is an immutable distributed collection of data with named columns. It is similar to a table in SQL.
* DataFrames are designed to process a large collection of structured data such as relational database and semi-structured data such as JSON
* DataFrame API currently supports several languages such as Python, R, Scala, and Java.
* DataFrames allows PySpark to query data using SQL, for example (SELECT * from table) or using the expression method for example (df-dot-select).

### 1.1 SparkSession - Entry Point for DataFrame API
* Previously you have learned about SparkContext which is the main entry point for creating RDDs.
* Similarly, SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame API.
* The SparkSession does for DataFrames what the SparkContext does for RDDs.
* A SparkSession can be used to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables etc.
* Similar to SparkContext, SparkSession is exposed to the PySpark shell as variable spark

### 1.2 Creating DataFrames in PySpark
* DataFrames in PySpark can be created in two main ways:
    * From an existing RDD using SparkSession's createDataFrame() method
    * From different data sources such as CSV, JSON, TXT using SparkSession's read method.
* Before going into the details of creating DataFrames, let's understand what schema is.
* Schema is the structure of data in DataFrame and helps Spark to optimize queries on the data more efficiently.
* A schema provides informational detail such as the column name, the type of data in that column, and whether null or empty values are allowed in the column.

(a) Create a DataFrame from RDD
* To create a DataFrame from an RDD, we will need to pass an RDD and a schema into SparkSession's createDataFrame method
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/d060f529-fbbb-48e2-be0c-a8d2df88305a)

(b) Create a DataFrame from reading CSV/JSON/TXT files
* To create a DataFrame from CSV/JSON/TXT files, we will make use of the SparkSession's spark.read property. Here is an example of creating df_csv DataFrame from people.csv file using spark.read.csv method:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/c95357f2-2837-4b40-81b6-940fe6cfecb3)

