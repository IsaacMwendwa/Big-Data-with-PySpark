![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/365c6bb6-10fe-4411-bafc-2e86a0b32b69)# Part 2: Big Data Fundamentals - PySpark SQL & DataFrames
* PySpark SQL is a Spark module for structured data processing.
* It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine
* Unlike the PySpark RDD API, PySpark SQL provides more information about the structure of data and the computation being performed

## 1. Abstracting Data with PySpark DataFrames 
* A DataFrame is an immutable distributed collection of data with named columns. It is similar to a table in SQL.
* DataFrames are designed to process a large collection of structured data such as relational database and semi-structured data such as JSON
* DataFrame API currently supports several languages such as Python, R, Scala, and Java.
* DataFrames allows PySpark to query data using SQL, for example (SELECT * from table) or using the expression method for example (df-dot-select).

### 1.1 SparkSession - Entry Point for DataFrame API
* Previously you have learned about SparkContext which is the main entry point for creating RDDs.
* Similarly, SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame API.
* The SparkSession does for DataFrames what the SparkContext does for RDDs.
* A SparkSession can be used to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables etc.
* Similar to SparkContext, SparkSession is exposed to the PySpark shell as variable spark

### 1.2 Creating DataFrames in PySpark
* DataFrames in PySpark can be created in two main ways:
    * From an existing RDD using SparkSession's createDataFrame() method
    * From different data sources such as CSV, JSON, TXT using SparkSession's read method.
* Before going into the details of creating DataFrames, let's understand what schema is.
* Schema is the structure of data in DataFrame and helps Spark to optimize queries on the data more efficiently.
* A schema provides informational detail such as the column name, the type of data in that column, and whether null or empty values are allowed in the column.

(a) Create a DataFrame from RDD
* To create a DataFrame from an RDD, we will need to pass an RDD and a schema into SparkSession's createDataFrame method
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/d060f529-fbbb-48e2-be0c-a8d2df88305a)

(b) Create a DataFrame from reading CSV/JSON/TXT files
* To create a DataFrame from CSV/JSON/TXT files, we will make use of the SparkSession's spark.read property. Here is an example of creating df_csv DataFrame from people.csv file using spark.read.csv method:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/c95357f2-2837-4b40-81b6-940fe6cfecb3)

## 2. Operating on DataFrames in PySpark
* Just like RDDs, DataFrames also support both transformations and actions.
* Let's explore some of the most common DataFrame Transformations such as select, filter, groupby, orderby, dropDuplicates, withColumnRenamed
* We will also look at some common DataFrame Actions such as printSchema, show, count, columns and describe

(a) select() and show() operations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/412a091c-cec3-4709-b3b3-cf3ca02ce416)

(b) groupby() and count() operations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/5a7e3d11-6a8f-4410-a214-78fd22a3c171)

(c) orderBy() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0474a930-25d7-47a3-9073-d3c798fa8ae6)

(d) dropDuplicates() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0cd86e17-ab1e-4665-baee-68d73085be7c)

(e) withColumnRenamed() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0feb768b-0e02-4b27-962a-893203eedc9f)

(f) printSchema() Action
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4817b4fc-643b-4d50-beb3-3a6d4c99053d)

(g) columns Action
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4561b8b8-79cc-4ff6-88f3-e92df481a2d8)

(h) describe() Action
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/1a7c5eee-9670-40ef-bb10-529cd1d1b23e)

(i) filter()
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/ffbba71c-58a1-43dc-8b4f-f77319ec5364)
 
## 3. Interacting with DataFrames using PySpark SQL
* Previously, you have seen how to interact with PySparkSQL using DataFrame API
* Next, we explore how to interact with PySparkSQL using SQL query

### 3.1 DataFrame API vs SQL queries
* In addition to DataFrame API, PySpark SQL allows you to manipulate DataFrames with SQL queries.
* What you can do using DataFrames API, can be done using SQL queries and vice versa.
* So what are the differences between DataFrames API and SQL queries?
* The DataFrames API provides a programmatic interface – basically a domain-specific language (DSL) for interacting with data.
* DataFrame queries are much easier to construct programmatically.
* Plain SQL queries can be significantly more concise and easier to understand. They are also portable and can be used without any modifications with every supported language.
* Many of the DataFrame operations that you have seen in the previous chapter, can be done using SQL queries.

### 3.2 Executing SQL Queries
* The SparkSession provides a method called sql which can be used to execute a SQL query.
* The sql method takes a SQL statement as an argument and returns a DataFrame representing the result of the given query:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/0560f8f0-b281-462b-9010-6d7fa888ed40)

Example Queries:

(a) SQL query to extract data
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/8637f543-644d-44a9-977a-40604ebb0063)

(b) Summarizing and Grouping Data using SQL Query
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/61265166-619a-41c6-8557-e15711746100)

(c) Filtering Columns using SQL Query
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/7eee8539-aab5-401e-beb2-6840730b8dbb)

## 4. Data Visualization in PySpark using DataFrames
* Data visualization is the way of representing your data in form of graphs or charts. It is considered a crucial component of Exploratory Data Analysis (EDA).
* Several open source tools exist to aid visualization in Python such as matplotlib, Seaborn, Bokeh etc.
* However, none of these visualization tools can be used directly with PySpark's DataFrames.
*  Currently, there are three different methods available to create charts using PySpark DataFrames - pyspark_dist_explore library, toPandas method, and HandySpark toPandas

(a) Data Visualization using Pyspark_dist_explore 
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/9e35a63f-3a17-42bc-9fcf-8636657827ef)

(b) Using Pandas for plotting DataFrames: toPandas()
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/05bcb026-4707-433e-b2e5-d76b6d13ca68)

Before we look at the third method, let's take a look at the differences between Pandas vs Spark DataFrames: 
   * But, Pandas won’t work in every case. It is a single machine tool and constrained by single machine limits.
   * So their size is limited by your server memory, and you will process them with the power of a single server.
   * In contrast, operations on Pyspark DataFrames run parallel on different nodes in the cluster.
   * In pandas DataFrames, we get the result as soon as we apply any operation Whereas operations in PySpark DataFrames are lazy in nature.
   * You can change a Pandas DataFrame using methods. We can’t change a PySpark DataFrame due to its immutable property.
   * Finally, the Pandas API supports more operations than PySpark DataFrames.

(c) HandySpark method of visualization
* HandySpark libary, is a relatively a new package.
* HandySpark is designed to improve PySpark user experience, especially when it comes to exploratory data analysis, including visualization capabilities.
* It makes fetching data or computing statistics for columns really easy, returning pandas objects straight away.
* It brings the long-missing capability of plotting data while retaining the advantage of performing the distributed computation:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/5aea62c6-2870-4e12-b08d-82065157b94a)
