# Cleaning Data with PySpark: Part 2

## 1. Improving Performance

### 1.1 Caching
* Caching in Spark refers to storing the results of a DataFrame in memory or on disk of the processing nodes in a cluster.
* Caching improves the speed for subsequent transformations or actions as the data likely no longer needs to be retrieved from the original data source.
* Using caching reduces the resource utilization of the cluster - there is less need to access the storage, networking, and CPU of the Spark nodes as the data is likely already present

Disadvantages of Caching
* Very large data sets may not fit in the memory reserved for cached DataFrames
  * Depending on the later transformations requested, the cache may not do anything to help performance.
* Local disk based cahing may not be a perfomance improvement
  * If a data set does not stay cached in memory, it may be persisted to disk
  * Depending on the disk configuration of a Spark cluster, this may not be a large performance improvement.
  * If you're reading from a local network resource and have slow local disk I/O, it may be better to avoid caching the objects.
* Cached objects may not be available
  * The lifetime of a cached object is not guaranteed
  * Spark handles regenerating DataFrames for you automatically, but this can cause delays in processing

Caching Tips
* Caching is incredibly useful, but only if you plan to use the DataFrame again. If you only need it for a single task, it's not worth caching
* The best way to gauge performance with caching is to test various configurations. Try caching your DataFrames at various points in the processing cycle and check if it improves your processing time.
* Try to cache in memory or fast NVMe / SSD storage. While still slower than main memory modern SSD based storage is drastically faster than spinning disk.
* Local spinning hard drives can still be useful if you are processing large DataFrames that require a lot of steps to generate, or must be accessed over the Internet. Testing this is crucial.
* If normal caching doesn't seem to work, try creating intermediate Parquet representations like we did earlier. These can provide a checkpoint in case a job fails mid-task and can still be used with caching to further improve performance
* You can manually stop caching a DataFrame when you're finished with it. This frees up cache resources for other DataFrames

Implementing Caching in PySpark
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/f54c0e20-6ccc-4086-ab04-f7c76a526c6c)
* A couple other options are available with caching in Spark:
  * To check if a DataFrame is cached, use the .is_cached boolean property which returns True (as in this case) or False
  * To un-cache a DataFrame, we call .unpersist() with no arguments. This removes the object from the cache
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/fb2d524a-64b6-4e54-addb-3d11f1270d62)

### 1.2 Improve Import Perfomance
Introduction to Spark Clusters
* Spark clusters consist of two types of processes - one driver process and as many worker processes as required.
* The driver handles task assignments and consolidation of the data results from the workers.
* The workers typically handle the actual transformation / action tasks of a Spark job.
* Once assigned tasks, they operate fairly independently and report results back to the driver.
* It is possible to have a single node Spark cluster, but you'll rarely see this in a production environment. There are different ways to run Spark clusters - the method used depends on your specific environment.

(a) Improving Import Perfomance
* When importing data to Spark DataFrames, it's important to understand how the cluster implements the job.
* The process varies depending on the type of task, but it's safe to assume that the more import objects available, the better the cluster can divvy up the job.
* This may not matter on a single node cluster, but with a larger cluster each worker can take part in the import process.
* In clearer terms, one large file will perform considerably worse than many smaller ones. 
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/c62df700-57ec-47a2-9119-aad020d1d022)

(b) Improving Performance using Schemas
* Well-defined schemas in Spark drastically improve import performance.
* Without a schema defined, import tasks require reading the data multiple times to infer structure. This is very slow when you have a lot of data.
* Spark may not define the objects in the data the same as you would.
* Spark schemas also provide validation on import. This can save steps with data cleaning jobs and improve the overall processing time.

(c) Improving Performance by Splitting Files
* There are various effective ways to split an object (files mostly) into more smaller objects.
* The first is to use built-in OS utilities such as split, cut, or awk.
  * An example using split uses the -l argument with the number of lines to have per file (10000 in this case).
  * The -d argument tells split to use numeric suffixes. The last two arguments are the name of the file to be split and the prefix to be used.
  * Assuming 'largefile' has 10M records, we would have files named chunk-0000 through chunk-9999.
* Another method is to use python (or any other language) to split the objects up as we see fit.
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/3e64c28d-eaf2-43c2-93ac-b020afdee851)

### 1.3 Cluster Configurations
(a) Configuration options
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/a32abc18-e9c5-4937-8fe5-c951b7a8f871)

(b) Cluster Types
* Spark deployments can vary depending on the exact needs of the users.
* One large component of a deployment is the cluster management mechanism.
* Spark clusters can be:
  * Single node clusters, deploying all components on a single system (physical / VM / container)
  * Standalone clusters, with dedicated machines as the driver and workers.
  * Managed clusters, meaning that the cluster components are handled by a third party cluster manager such as YARN, Mesos, or Kubernetes.
* In this course, we're using a single node cluster. Your production environment can vary wildly, but we'll discuss standalone clusters as the concepts flow across all management types.

(c) Driver
* There is one driver per Spark cluster.
* The driver is responsible for several things, including the following:
  * Handling task assignment to the various nodes / processes in the cluster, which also includes monitoring the state of all processes and tasks and handles any task retries
  * Consolidating results from the other processes in the cluster
  * The driver handles any access to shared data and verifies each worker process has the necessary resources (code, data, etc).
* Given the importance of the driver, it is often worth increasing the specifications of the node compared to other systems. Doubling the memory compared to other nodes is recommended.
* This is useful for task monitoring and data consolidation tasks. As with all Spark systems, fast local storage is useful for running Spark in an ideal setup.

(d) Worker
* A Spark worker handles running tasks assigned by the driver and communicates those results back to the driver.
* Ideally, the worker has a copy of all code, data, and access to the necessary resources required to complete a given task.
* If any of these are unavailable, the worker must pause to obtain the resources.
* When sizing a cluster, there are a few recommendations:
  * Depending on the type of task, more worker nodes is often better than larger nodes. This can be especially obvious during import and export operations as there are more machines available to do the work.
  * As with everything in Spark, test various configurations to find the correct balance for your workload. Assuming a cloud environment, 16 worker nodes may complete a job in an hour and cost $50 in resources. An 8 worker configuration might take 1.25 hrs but cost only half as much.
  * Workers can make use of fast local storage (SSD / NVMe) for caching, intermediate files, etc.

Example 1: Reading Spark Configurations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/ef2749ab-196e-4b56-901a-24c5caa1ba4b)

Example 2: Writing Spark Configurations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/c380d313-a8fb-4917-b2b0-53169623cd92)

### 1.4 Improving Performance of Spark Tasks in General
