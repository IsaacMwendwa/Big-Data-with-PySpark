# Cleaning Data with PySpark: Part 2

## 1. Improving Performance

### 1.1 Caching
* Caching in Spark refers to storing the results of a DataFrame in memory or on disk of the processing nodes in a cluster.
* Caching improves the speed for subsequent transformations or actions as the data likely no longer needs to be retrieved from the original data source.
* Using caching reduces the resource utilization of the cluster - there is less need to access the storage, networking, and CPU of the Spark nodes as the data is likely already present

Disadvantages of Caching
* Very large data sets may not fit in the memory reserved for cached DataFrames
  * Depending on the later transformations requested, the cache may not do anything to help performance.
* Local disk based cahing may not be a perfomance improvement
  * If a data set does not stay cached in memory, it may be persisted to disk
  * Depending on the disk configuration of a Spark cluster, this may not be a large performance improvement.
  * If you're reading from a local network resource and have slow local disk I/O, it may be better to avoid caching the objects.
* Cached objects may not be available
  * The lifetime of a cached object is not guaranteed
  * Spark handles regenerating DataFrames for you automatically, but this can cause delays in processing

Caching Tips
* Caching is incredibly useful, but only if you plan to use the DataFrame again. If you only need it for a single task, it's not worth caching
* The best way to gauge performance with caching is to test various configurations. Try caching your DataFrames at various points in the processing cycle and check if it improves your processing time.
* Try to cache in memory or fast NVMe / SSD storage. While still slower than main memory modern SSD based storage is drastically faster than spinning disk.
* Local spinning hard drives can still be useful if you are processing large DataFrames that require a lot of steps to generate, or must be accessed over the Internet. Testing this is crucial.
* If normal caching doesn't seem to work, try creating intermediate Parquet representations like we did earlier. These can provide a checkpoint in case a job fails mid-task and can still be used with caching to further improve performance
* You can manually stop caching a DataFrame when you're finished with it. This frees up cache resources for other DataFrames

Implementing Caching in PySpark
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/f54c0e20-6ccc-4086-ab04-f7c76a526c6c)
* A couple other options are available with caching in Spark:
  * To check if a DataFrame is cached, use the .is_cached boolean property which returns True (as in this case) or False
  * To un-cache a DataFrame, we call .unpersist() with no arguments. This removes the object from the cache
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/fb2d524a-64b6-4e54-addb-3d11f1270d62)

### 1.2 Improve Import Perfomance
Introduction to Spark Clusters
* Spark clusters consist of two types of processes - one driver process and as many worker processes as required.
* The driver handles task assignments and consolidation of the data results from the workers.
* The workers typically handle the actual transformation / action tasks of a Spark job.
* Once assigned tasks, they operate fairly independently and report results back to the driver.
* It is possible to have a single node Spark cluster, but you'll rarely see this in a production environment. There are different ways to run Spark clusters - the method used depends on your specific environment.

(a) Improving Import Perfomance
* When importing data to Spark DataFrames, it's important to understand how the cluster implements the job.
* The process varies depending on the type of task, but it's safe to assume that the more import objects available, the better the cluster can divvy up the job.
* This may not matter on a single node cluster, but with a larger cluster each worker can take part in the import process.
* In clearer terms, one large file will perform considerably worse than many smaller ones. 
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/c62df700-57ec-47a2-9119-aad020d1d022)

(b) Improving Performance using Schemas
* Well-defined schemas in Spark drastically improve import performance.
* Without a schema defined, import tasks require reading the data multiple times to infer structure. This is very slow when you have a lot of data.
* Spark may not define the objects in the data the same as you would.
* Spark schemas also provide validation on import. This can save steps with data cleaning jobs and improve the overall processing time.

(c) Improving Performance by Splitting Files
* There are various effective ways to split an object (files mostly) into more smaller objects.
* The first is to use built-in OS utilities such as split, cut, or awk.
  * An example using split uses the -l argument with the number of lines to have per file (10000 in this case).
  * The -d argument tells split to use numeric suffixes. The last two arguments are the name of the file to be split and the prefix to be used.
  * Assuming 'largefile' has 10M records, we would have files named chunk-0000 through chunk-9999.
* Another method is to use python (or any other language) to split the objects up as we see fit.
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/3e64c28d-eaf2-43c2-93ac-b020afdee851)

### 1.3 Cluster Configurations
(a) Configuration options
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/a32abc18-e9c5-4937-8fe5-c951b7a8f871)

(b) Cluster Types
* Spark deployments can vary depending on the exact needs of the users.
* One large component of a deployment is the cluster management mechanism.
* Spark clusters can be:
  * Single node clusters, deploying all components on a single system (physical / VM / container)
  * Standalone clusters, with dedicated machines as the driver and workers.
  * Managed clusters, meaning that the cluster components are handled by a third party cluster manager such as YARN, Mesos, or Kubernetes.
* In this course, we're using a single node cluster. Your production environment can vary wildly, but we'll discuss standalone clusters as the concepts flow across all management types.

(c) Driver
* There is one driver per Spark cluster.
* The driver is responsible for several things, including the following:
  * Handling task assignment to the various nodes / processes in the cluster, which also includes monitoring the state of all processes and tasks and handles any task retries
  * Consolidating results from the other processes in the cluster
  * The driver handles any access to shared data and verifies each worker process has the necessary resources (code, data, etc).
* Given the importance of the driver, it is often worth increasing the specifications of the node compared to other systems. Doubling the memory compared to other nodes is recommended.
* This is useful for task monitoring and data consolidation tasks. As with all Spark systems, fast local storage is useful for running Spark in an ideal setup.

(d) Worker
* A Spark worker handles running tasks assigned by the driver and communicates those results back to the driver.
* Ideally, the worker has a copy of all code, data, and access to the necessary resources required to complete a given task.
* If any of these are unavailable, the worker must pause to obtain the resources.
* When sizing a cluster, there are a few recommendations:
  * Depending on the type of task, more worker nodes is often better than larger nodes. This can be especially obvious during import and export operations as there are more machines available to do the work.
  * As with everything in Spark, test various configurations to find the correct balance for your workload. Assuming a cloud environment, 16 worker nodes may complete a job in an hour and cost $50 in resources. An 8 worker configuration might take 1.25 hrs but cost only half as much.
  * Workers can make use of fast local storage (SSD / NVMe) for caching, intermediate files, etc.

Example 1: Reading Spark Configurations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/ef2749ab-196e-4b56-901a-24c5caa1ba4b)

Example 2: Writing Spark Configurations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/c380d313-a8fb-4917-b2b0-53169623cd92)

### 1.4 Improving Performance of Spark Tasks in General
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/cec08946-7322-40bc-9cbb-7f9b08d3ff13)

(a) What is Shuffling
* Spark distributes data amongst the various nodes in the cluster.
* A side effect of this is what is known as shuffling.
* Shuffling is the moving of data fragments to various workers as required to complete certain tasks.
* Shuffling is useful and hides overall complexity from the user (the user doesn't have to know which nodes have what data).
* That being said, it can be slow to complete the necessary transfers, especially if a few nodes require all the data.
* Shuffling lowers the overall throughput of the cluster as the workers must spend time waiting for the data to transfer.
* This limits the amount of available workers for the remaining tasks in the system.
* Shuffling is often a necessary component, but it's helpful to try to minimize it as much as possible.

(b) Limiting Shuffling
* It can be tricky to remove shuffling operations entirely but there are a few things that can limit it.
* Using .coalesce() instead of .repartition()
  * The DataFrame .repartition() function takes a single argument, the number of partitions requested. We've used this in an earlier chapter to illustrate the effect of partitions with the monotonically_increasing_id() function.
  * Repartitioning requires a full shuffle of data between nodes & processes and is quite costly. If you need to reduce the number of partitions, use the .coalesce() function instead.
  * It takes a number of partitions smaller than the current one and consolidates the data without requiring a full data shuffle.
  * Note: calling .coalesce() with a larger number of partitions does not actually do anything.
* Using .broadcast() with join()
  * The .join() function is a great use of Spark and provides a lot of power.
  * Calling .join() indiscriminately can often cause shuffle operations, leading to increased cluster load & slower processing times.
  * To avoid some of the shuffle operations when joining Spark DataFrames you can use the .broadcast() function
* Finally, an important note about data cleaning operations is remembering to optimize for what matters. The speed of your initial code may be perfectly acceptable and time may be better spent elsewhere.

Broadcasting
* Broadcasting in Spark is a method to provide a copy of an object to each worker.
* When each worker has its own copy of the data, there is less need for communication between nodes.
* This limits data shuffles and it's more likely a node will fulfill tasks independently
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/47cd3c11-4460-4548-a5e4-1a43748fd042)

Example Execution Plan of Normal Join
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/1b1efb15-3d79-45d5-8791-00fb82d3bee9)

Example Execution Plan of Broadcast Join
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/9e274bef-40c5-4b1d-8834-a009f9fe9475)

Time Comparison: Normal vs. Broadcast Joins
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/1fcec067-cddb-47d4-b53d-250a2d7f0da9)


## 2. Complex Processing and Data Pipelines

### 2.1 Intro to Data Pipelines
* Data pipelines are the set of steps needed to move from an input data source, or sources, and convert it to the desired output. A data pipeline can consist of any number of steps or components, and can span many systems.
* For our purposes, we’ll be setting up a data pipeline within Spark, but realize that a full production data pipeline will likely communicate with many systems.

Components of a Data Pipeline
* Much like Spark in general, a data pipeline typically consists of inputs, transformations, and the outputs of those steps. In addition, there is often validation and analysis steps before delivery of the data to the next user.
* An input can be any of the data types we've looked at so far, including CSV, JSON, text, etc. It could be from the local file system, or from web services, APIs, databases, and so on. The basic idea is to read the data into a DataFrame as we've done previously.
* Once we have the data in a DataFrame, we need to transform it in some fashion. You’ve done the individual steps several times throughout this course - adding columns, filtering rows, performing calculations as needed. In our previous examples, we’ve only done one or two of these steps at a time but a pipeline can consist of as many of these steps as needed so we can format the data into our desired output.
* After we’ve defined our transformations we need to output the data into a usable form. You’ve already written files out to CSV or Parquet format, but it could include multiple copies with various formats, or instead write the output to a database, a web service, etc.
* The last two steps vary greatly depending on your needs. We'll discuss validation in a later lesson, but the idea is to run some form of testing on the data to verify it is as expected. Analysis is often the final step before handing the data off to the next user. This can include things such as row counts, specific calculations, or pretty much anything that makes it easier for the user to consume the dataset.

Pipeline Details
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/3097c0b8-1be2-4130-8d1a-fd1f10b773c0)

### 2.2 Data handling techniques
* When reading data into Spark, you're rarely given a fully uniform file. Often there is content that needs to be removed or reformatted.
* Some common issues include: Incorrect data, consisting of empty rows, commented lines, headers, or even rows that don't match the intended schema.
* Real world data often includes nested structures, including columns that use different delimiters. This could include the primary columns separated via a comma, but including some components separated via a semi-colon.
* Real data often won't fit into a tabular format, sometimes consisting of a differing number of columns per row.
* There are various ways to parse data in all of these situations. The way you choose will depend on your specific needs. We are focusing on CSV data for this course, but the general scenarios described apply to other formats as well.

Removing Blank lines, headers, and comments
* Spark's CSV parser can handle many common data issues via optional parameters. Blank lines are automatically removed (unless specifically instructed otherwise) when using the CSV parsing.
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4d8bdc77-aa8b-4ca9-9f5b-e1fbc95112fe)

Automatic Column Creation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/dae95dff-541d-4e18-8a79-beb2c56db38a)

Example Problem: Data Handling
* (i) Removing comments:
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/f364b7e5-2613-4fe0-b286-d677a962d8e8)
* (ii) Removing invalid rows:
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/a99adcdd-e8cb-42b9-8731-d18fc007ff56)
* (iii) Splitting into Columns
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/61448e78-79df-4771-b768-dc6dfc25b99d)
* (iv) Futher Parsing
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/9116da5c-f64c-425b-bf9c-9b8efe21c6d6)

### 2.3 Data Validation
* Data validation is verifying that a dataset complies with an expected format. This can include verifying that the number of rows and columns is as expected.
* For example, is the row count within 2% of the previous month's row count? Another common test is do the data types match?
* If not specifically validated with a schema, does the content meet the requirements (only 9 characters or less, etc).
* Finally, you can validate against more complex rules. This includes verifying that the values of a set of sensor readings are within physically possible quantities

(a) Validating via joins
* One technique used to validate data in Spark is using joins to verify the content of a DataFrame matches a known set.
* Validating via a join will compare data against a set of known values. This could be a list of known ids, companies, addresses, etc.
* Joins make it easy to determine if data is present in a set. This could be only rows that are in one DataFrame, present in both, or present in neither.
* Joins are also comparatively fast, especially vs validating individual rows against a long list of entries.
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/2376668d-d517-4777-b05d-e171631cd79d)
* Example Problem: (i) Validating via Joins
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/fd3a1532-b6f3-4a03-ae60-f3a169d6a6e2)
* (ii) Examining Invalid Rows
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/3de49e7e-c8d6-44be-aa02-88b8eb862cd9)


(b) Complex Rule Validation (CRV)
* Complex rule validation is the idea of using Spark components to validate logic.
* This may be as simple as using the various Spark calculations to verify the number of columns in an irregular data set. You've done something like this already in the previous lessons.
* The validation can also be applied against an external source: web service, local files, API calls.
* These rules are often implemented as a UDF to encapsulate the logic to one place and easily run against the content of a DataFrame.

### 2.4 Final Analysis and Delivery
(a) Analysis Calculations using UDFs
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/5235c4dc-0baf-4657-adb3-256169d07b46)

(b) Analysis Calculations (Inline)
* Spark UDFs are very powerful and flexible and are sometimes the only way to handle certain types of data.
* Unfortunately UDFs do come at a performance penalty compared to the built-in Spark functions, especially for certain operations.
* The solution is to perform calculations inline if possible. Spark columns can be defined using in-line math operations, which can then be optimized for the best performance.
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/001c053f-060f-496a-812b-ae5fc40c661b)

Example Analysis Problem
* (i) Defining Schema
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/f209c014-6171-40c5-bf99-5d4a34f1adc7)
* (ii) Per image count
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/303c076d-6171-4c14-93bc-8f6b370db3d2)
* (iii) Percentage dog pixels
  * To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. You can calculate the bounding box with the formula: (Xend - Xstart) * (Yend - Ystart)
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/fa3b3172-1f3e-425d-8480-30f39b8c5624)
