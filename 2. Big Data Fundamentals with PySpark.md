# 1. Introduction to Big Data
* Big data is a term used to refer to the study and applications of data sets that are too complex for traditional data-processing software
* There are three Vs of Big data that are used to describe its characteristics:
  * Volume refers to the size of data
  * Variety refers to different sources and formats of data
  * Velocity is the speed at which data is generated and available for processing

### 1.1 Big Data Concepts and Terminology
* Clustered computing is the pooling of resources of multiple machines to complete jobs
* Parallel computing is a type of computation in which many calculations are carried out simultaneously
* A distributed computing involves nodes or networked computers that run jobs in parallel
* Batch processing refers to the breaking data into smaller pieces and running each piece on an individual machine
* Real-time processing demands that information is processed and made ready immediately

### 1.2 Big Data Processing Systems
* There are two popular frameworks for Big Data processing:
  * The first is the highly successful Hadoop/MapReduce framework. Hadoop/MapReduce framework is open source and scalable framework for batch data
  *  The second is the most popular Apache Spark which is a parallel framework for storing and processing of Big Data across clustered computers. It is also open source and is suited for both batch and real-time data processing
* The main features of Apache Spark are:
  * Spark distributes data and computation across multiple computers executing complex multi-stage applications such as machine learning
  * Spark runs most computations in memory and thereby provides better performance for applications such as interactive data mining
  * Spark helps to run an application up to 100 times faster in memory, and 10 times faster when running on disk
  * Spark is mainly written in Scala language but also have support for Java, Python, R, and SQL

### 1.3 Apache Spark Components
* Apache Spark is a powerful alternative to Hadoop MapReduce, with rich features like machine learning, real-time stream processing, and graph computations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/214e3cd2-d688-49d3-83bf-a5a491dc5dcf)
* At the center of the ecosystem is the Spark Core which contains the basic functionality of Spark, and the rest of Sparkâ€™s libraries are built on top of it
* First is Spark SQL, which is a library for processing structured and semi-structured data in Python, Java, and Scala
* The second is MLlib, which is a library of common machine learning algorithms
* The third component is GraphX, which is a collection of algorithms and tools for manipulating graphs and performing parallel graph computations
* Finally, Spark Streaming is a scalable, high-throughput processing library for real-time data

### 1.4 Spark Modes of Deployment
* Spark can be run on two modes:
  * Local mode where you can run Spark on a single machine such as your laptop. The local mode is very convenient for testing, debugging and demonstration purposes
  * Cluster mode where Spark is run on a cluster. The cluster mode is mainly used for production.
* The development workflow is that you start on local mode and transition to cluster mode. During the transition from local to cluster mode, no code change is necessary
