# 1. Introduction to Big Data analysis with Spark
* Big data is a term used to refer to the study and applications of data sets that are too complex for traditional data-processing software
* There are three Vs of Big data that are used to describe its characteristics:
  * Volume refers to the size of data
  * Variety refers to different sources and formats of data
  * Velocity is the speed at which data is generated and available for processing

### 1.1 Big Data Concepts and Terminology
* Clustered computing is the pooling of resources of multiple machines to complete jobs
* Parallel computing is a type of computation in which many calculations are carried out simultaneously
* A distributed computing involves nodes or networked computers that run jobs in parallel
* Batch processing refers to the breaking data into smaller pieces and running each piece on an individual machine
* Real-time processing demands that information is processed and made ready immediately

### 1.2 Big Data Processing Systems
* There are two popular frameworks for Big Data processing:
  * The first is the highly successful Hadoop/MapReduce framework. Hadoop/MapReduce framework is open source and scalable framework for batch data
  *  The second is the most popular Apache Spark which is a parallel framework for storing and processing of Big Data across clustered computers. It is also open source and is suited for both batch and real-time data processing
* The main features of Apache Spark are:
  * Spark distributes data and computation across multiple computers executing complex multi-stage applications such as machine learning
  * Spark runs most computations in memory and thereby provides better performance for applications such as interactive data mining
  * Spark helps to run an application up to 100 times faster in memory, and 10 times faster when running on disk
  * Spark is mainly written in Scala language but also have support for Java, Python, R, and SQL

### 1.3 Apache Spark Components
* Apache Spark is a powerful alternative to Hadoop MapReduce, with rich features like machine learning, real-time stream processing, and graph computations
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/214e3cd2-d688-49d3-83bf-a5a491dc5dcf)
* At the center of the ecosystem is the Spark Core which contains the basic functionality of Spark, and the rest of Spark’s libraries are built on top of it
* First is Spark SQL, which is a library for processing structured and semi-structured data in Python, Java, and Scala
* The second is MLlib, which is a library of common machine learning algorithms
* The third component is GraphX, which is a collection of algorithms and tools for manipulating graphs and performing parallel graph computations
* Finally, Spark Streaming is a scalable, high-throughput processing library for real-time data

### 1.4 Spark Modes of Deployment
* Spark can be run on two modes:
  * Local mode where you can run Spark on a single machine such as your laptop. The local mode is very convenient for testing, debugging and demonstration purposes
  * Cluster mode where Spark is run on a cluster. The cluster mode is mainly used for production.
* The development workflow is that you start on local mode and transition to cluster mode. During the transition from local to cluster mode, no code change is necessary

## 1.5. PySpark: Spark with Python
* Apache Spark provides high-level APIs in Scala, Java, Python, and R
* Apache Spark is originally written in Scala programming language. To support Python with Spark, PySpark was developed
* Unlike previous versions, the newest version of PySpark provides computation power similar to Scala
* APIs in PySpark are similar to Pandas & Scikit-learn Python packages. Thus, the entry level barrier to PySpark is very low for beginners

### 1.5.1 Spark Shell
* Spark comes with interactive shells that enable ad-hoc data analysis
* Spark shell is an interactive environment through which one can access Spark's functionality quickly and conveniently. Spark shell is particularly helpful for fast interactive prototyping before running the jobs on clusters
* Unlike most other shells, Spark shell allow you to interact with data that is distributed on disk or in memory across many machines, and Spark takes care of automatically distributing this processing.
* Spark provides the shell in three programming languages: Spark-shell for Scala, PySpark-shell for Python and SparkR for R.
* The PySpark-shell is the Python-based command line tool to develop Spark's interactive applications in Python. PySpark helps data scientists interface with Spark data structures in Apache Spark and Python

### 1.5.2 Understanding SparkContext
* In order to interact with Spark using PySpark shell, you need an entry point.
* SparkContext is an entry point to interact with underlying Spark functionality
* Before understanding SparkContext, let’s understand what an entry point is. An entry point is where control is transferred from the Operating system to the provided program
* In simpler terms, it's like a key to your house. Without the key you cannot enter the house, similarly, without an entry point, you cannot run any PySpark jobs
* A SparkContext represents the entry point to Spark functionality. It's like a key to your car. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here.
* PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable sc
* You can access the SparkContext in the PySpark shell as a variable named sc
* Now let's take a look at some of the important attributes of SparkContext:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/3a62ee63-c907-4fe4-92c3-f759863738cf)

### 1.6 Review of Functional Programming in Python
* Understanding PySpark becomes a lot easier if we understand functional programming principles in Python
* We will review some of the Python functions such as lambda, map and filter
* Python supports the creation of anonymous functions (functions that are not bound to a name at runtime), using a construct called the lambda
* lambda functions are very powerful, well integrated into Python, and are often used in conjunction with typical functional concepts like map and filter functions
* Like def, the lambda creates a function to be called later in the program. However, it returns the function instead of assigning it to a name. This is why lambdas are known as anonymous functions
* In practice, they are used as a way to inline a function definition, or to defer execution of a code. Lambda functions can be used

### 1.6.1 Lambda function syntax
* Lambda functions can be used whenever function objects are required
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/024cbf53-3f15-4931-b647-ad6dcfad10e2)
* Difference between def and lambda functions:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/c6253bef-39c5-4404-8975-b47423cdfcea)
* Using map() with a lambda function:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/3c158248-a373-40fb-b40d-eb1c3e38b0c7)
* Using filter() with a lambda function:
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/8e72fa26-680d-48be-911b-4959dc66467e)


# 2. Programming in PySpark RDD’s
### 2.1 Abstracting Data with RDDs
* RDD stands for Resilient Distributed Datasets. It is simply an immutable collection of data distributed across the cluster
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/b03c7d5e-d70a-4298-b7d4-eccae5dd7e3f)
* RDD is the main abstraction that Spark provides, and is the fundamental and backbone data type in PySpark
* When Spark starts processing data, it divides the data into partitions and distributes the data across cluster nodes, with each node containing a slice of data
* RDDs contain three main features:
  * Resilient - the ability to withstand failures and recompute missing or damaged partitions
  * Distributed - spanning the jobs across multiple nodes in the cluster for efficient computation
  * Datasets - is a collection of partitioned data e.g. Arrays, Tables, Tuples or other objects

#### 2.1.1 Creating RDDs (3 Ways)
1. The simplest method to create RDDs is to take an existing collection of objects (eg. a list, an array or a set) and pass it to SparkContext’s parallelize method
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/a9383e15-b119-40a9-a62d-7a86c01173ca)
2. A more common way to create RDDs is to load data from external datasets such as files stored in HDFS or objects in Amazon S3 buckets or from lines in a text file stored locally and pass it to SparkContext's textFile method
   * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/fd20ad30-b011-4488-91f1-db21c357cb2a)
3. Finally, RDDs can also be created from existing RDDs

#### 2.1.2 Understanding Partitioning in PySpark
* Data partitioning is an important concept in Spark and understanding how Spark deals with partitions allow one to control parallelism.
* A partition in Spark is the division of the large dataset with each part being stored in multiple locations across the cluster.
* By default Spark partitions the data at the time of creating RDD based on several factors such as available resources, external datasets etc
* However, this behavior can be controlled by passing a second argument called minPartitions which defines the minimum number of partitions to be created for an RDD.
* The number of partitions in an RDD can always be found by using the getNumPartitions method:
  * ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/402eaf85-74dc-4165-9f4e-818e4c99c8d5)

## 2.2 Basic RDD Transformations and Actions
* PySpark supports two different types of operations - Transformations and Actions
* Transformations are operations on RDDs that return a new RDD
* Actions are operations that perform some computation on the RDD.

### 2.2.1 RDD Transformations
* The most important feature which helps RDDs in fault tolerance and optimizing resource use is the lazy evaluation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/80201a03-fc3a-48fa-b5e8-db8d4b73b86a)

(a) map() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/18dc8f1e-488f-4f79-b03e-20f43dabd6d2)

(b) filter() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/25d99f88-334c-4571-8d91-ca9fbd09c6a3)

(c) flatMap() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/e96bcf16-93eb-45a3-812f-74d6c1033e4d)

(d) union() Transformation
* ![image](https://github.com/IsaacMwendwa/Big-Data-with-PySpark/assets/51324520/4374a663-d1b0-4d86-a79b-d5fb266f0c25)

### 2.2.2 RDD Actions
* So far you have seen how RDD Transformations but after applying Transformations at some point, you'll want to actually do something with your dataset. This is when Actions come into picture.
* Actions are the operations that are applied on RDDs to return a value after running a computation
* There are four basic actions: collect(), take(), first() and count()
